
parameters:
 - name: model_name
   displayName: mode name on huggingface website
   type: string
   default: meta-llama/Llama-3.2-3B-Instruct

pool: onnxruntime-Linux-GPU-T4 #onnxruntime-tensorrt-linuxbuild-T4
steps:
 - task: UsePythonVersion@0
   inputs:
     versionSpec: '3.12'
     architecture: 'x64'

#  - bash: |
#      pip install -r llama-requirements.txt
#    displayName: install the necessary packages
#    workingDirectory: $(Build.SourcesDirectory)


#  - script: az --version
#    displayName: 'Show Azure CLI version'

#  - script: |
#       python -c "with open('hello.txt', 'w') as f:f.write('hello world')"
#       azcopy cp --recursive "./hello.txt" 'https://sunghchostorageaccount.blob.core.windows.net/test'
#       ls
#    displayName: 'try azcopy upload to sun blob'
 - bash: |
    nvidia-smi
   displayName: "dump nvidia-smi"
 
#  - bash: |
#     ls -l /usr
#     nvcc --version
#    displayName: "dump cuda version"

 - script: |
     azcopy cp --recursive "https://sunghchostorageaccount.blob.core.windows.net/phi-3/phi-3.5-mini-instruct/onnx/cuda/cuda-int4-rtn-block-32" './phi-3.5-mini-instruct-cuda-int4'
     ls -l ./phi-3.5-mini-instruct-cuda-int4/cuda-int4-rtn-block-32
   displayName: "download model from blob"
   workingDirectory: $(Build.SourcesDirectory)

 - script: |
     docker run --gpus all --rm \
        --ipc=host \
        --volume $(Build.SourcesDirectory):/ort_src \
        --volume $(Build.BinariesDirectory):/build \
        -p 8000:8000 \
        -e CCACHE_DIR=/cache -w /ort_src \
        ptebic.azurecr.io/public/aifx/acpt/stable-ubuntu2004-cu121-py310-torch222:biweekly.202410.2 /bin/bash /ort_src/docker_script.sh
   workingDirectory: $(Build.SourcesDirectory)
   displayName: "start model endpoint and RAI eval in container"


#  - script: |
#      pip install fastapi
#      pip install uvicorn
#      pip install onnxruntime-genai-cuda --upgrade
#      uvicorn main:app --reload
#    displayName: "start model endpoint"
#    workingDirectory: $(Build.SourcesDirectory)

#  - script: |
#      pip install azure-ai-evaluation --upgrade
#      pip install promptflow-azure --upgrade
#      python ./evaluate-models-target-IP.py
#    displayName: "run RAI script"
#    workingDirectory: $(Build.SourcesDirectory)

#  - bash: |
#      echo Authenticate with Huggingface repository 

#      huggingface-cli login --token $(hf_token)

#    displayName: "Authentication to Huggingface repo" 

 - bash: |
    pip list 
   displayName: "dump pip list"



#  - script: | 

#      python $(Build.SourcesDirectory)/llama_model_builder.py --model_name '${{ parameters.model_name }}' --output_dir '$(Build.BinariesDirectory)'

#    displayName: "Convert Huggingface model to ONNX model"
     
#    workingDirectory: $(Build.SourcesDirectory)

#  - task: AzureCLI@2
#    displayName: 'upload model to Blob Storage'
#    inputs:
#      azureSubscription: AIInfraBuild
#      scriptLocation: inlineScript
#      scriptType: bash
#      inlineScript: |
#        cd $(Build.BinariesDirectory)
#        azcopy copy './models/${{ parameters.model_name }}/output_model/model' 'https://sunghchostorageaccount.blob.core.windows.net/test' --recursive


#  - task: AzureCLI@2
#    displayName: 'upload model to AzureML registry'
#    inputs:
#      azureSubscription: AIInfraBuild
#      scriptLocation: inlineScript
#      scriptType: bash
#      inlineScript: |
#        cd $(Build.BinariesDirectory)
#        az extension add -n ml -y
#        az extension list
#        az account list
#        az ml data create --name llama32-3b-instruct-for-test-only --version 1 --path './models/${{ parameters.model_name }}/output_model/model' --registry-name model-publish-test --description "llama3.2-3b-instruct-test-only from pipeline"


#- bash: |
 #    pip install fastapi
 #    pip install uvicorn
 #    pip install onnxruntime-genai-cuda

 #  displayName: install fastapi packages

 #- script: | 

 #    uvicorn main:app --reload

 #  displayName: "Start model endpoint"
     
 #  workingDirectory: $(Build.SourcesDirectory)
